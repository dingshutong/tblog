{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/structuring-ML-projects","result":{"data":{"markdownRemark":{"id":"e2d3df63-51d7-53a5-b1f1-16d5dec907bc","html":"<h2 id=\"why-ml-strategy\" style=\"position:relative;\"><a href=\"#why-ml-strategy\" aria-label=\"why ml strategy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Why ML strategy</h2>\n<p>In order to improve your machine learning model, you might need to try out many different ideas. If you have a good ML strategy, it allows your to have quick and effective ways to figure out which of all of these ideas and maybe even other ideas, are worth pursuing and which ones you can safely discard. </p>\n<blockquote>\n<p>A number of strategies, that is, ways of analyzing a machine learning problem that will point   you in the direction of the most promising things to try. </p>\n</blockquote>\n<h2 id=\"orthogonalization-process\" style=\"position:relative;\"><a href=\"#orthogonalization-process\" aria-label=\"orthogonalization process permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Orthogonalization Process</h2>\n<p>Orthogonalization allows you to split and to separate different tuning <em>knobs</em> (procedures) to check its effect individually. </p>\n<h3 id=\"chain-of-assumptions-in-ml\" style=\"position:relative;\"><a href=\"#chain-of-assumptions-in-ml\" aria-label=\"chain of assumptions in ml permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chain of assumptions in ML</h3>\n<p>Fit the <strong>training set</strong> well on cost function? (Bigger network, Adam …)</p>\n<p>-></p>\n<p>Fit the <strong>dev set</strong> well on cost function? (Regularization, bigger training set)</p>\n<p>-></p>\n<p>Fit the <strong>test set</strong> well on cost function? (Bigger dev set)</p>\n<p>-></p>\n<p>Perform well in real world application (change dev set or cost function)</p>\n<h2 id=\"setting-up-your-goals\" style=\"position:relative;\"><a href=\"#setting-up-your-goals\" aria-label=\"setting up your goals permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Setting up your goals</h2>\n<p>Usually it is more effective to use a <strong>single number evaluation metric</strong> when comparing across different models and algorithms. If the evaluation metrics can be combined by some sort of averaging, we could use their harmonic mean as a single measure.</p>\n<p>However, it’s not always easy to combine all the things you care about into a single row number evaluation metric. In those cases it is sometimes useful to set up <strong>satisficing as well as optimizing metric</strong>. For example, </p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Accuracy</th>\n<th>Running time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>93%</td>\n<td>80ms</td>\n</tr>\n<tr>\n<td>B</td>\n<td>94%</td>\n<td>82ms</td>\n</tr>\n<tr>\n<td>C</td>\n<td>96%</td>\n<td>1800ms</td>\n</tr>\n</tbody>\n</table>\n<p>One way to create a single metric is to create a cost measure such as the overall cost is accuracy minus 0.5 times running time. Or a more easier way is to optimize the accuracy while satisficing at certain running speed. That is, for example to maximize Accuracy subject to Running time &#x3C;= 100ms. </p>\n<p>So if you have N metrics to look, you can choose the most important one you want to optimze, and the rest N-1 metrics as the subject-to satisficer. </p>\n<h2 id=\"traindevtest-distributions\" style=\"position:relative;\"><a href=\"#traindevtest-distributions\" aria-label=\"traindevtest distributions permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Train/dev/test distributions</h2>\n<p><strong>dev</strong> set is also called the development set, or sometimes it is called hold-out cross validation set. Ideally, the dev set and the test set should come from the same distribution.\nPull all your data and then randomly shuffle them and split them into dev/test. </p>\n<p>Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on.</p>\n<p>Set your test set to be big enough to give high confidence in the overall performance of your system. For some applications, maybe you don’t need a high confidence in the overall performance of your final system. Maybe all you need is a train and dev set, not having a test set might be okay. </p>\n<p>In the modern machine learning exercises, the trend has been to use more data for training and less for dev and test, especially when you have a very large data sets. Say you have over 1 million samples, to split them: we could have 98% train/ 1% dev/ 1% test. </p>\n<h2 id=\"when-to-change-devtest-sets-and-metrics\" style=\"position:relative;\"><a href=\"#when-to-change-devtest-sets-and-metrics\" aria-label=\"when to change devtest sets and metrics permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>When to change dev/test sets and metrics</h2>\n<p>Sometimes partway through a project you might realize you put your target in the wrong place. In that case you should move your target. </p>\n<p>For example, if it is doing well on your metric + dev/test sets does not correspond to doing well on your application, change your metric and/or dev/test sets.</p>\n<p>If your current metric and data you are evaluating on doesn’t correspond to doing well on what you actually care about, then change your metrics and/or your dev/test set to better capture what you need your algorithm to actually do well on.</p>\n<h2 id=\"error-analysis\" style=\"position:relative;\"><a href=\"#error-analysis\" aria-label=\"error analysis permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Error analysis</h2>\n<p><strong>“Ceiling on performance”</strong> Suppose that it turns out that 5% of your 100 mislabeled dev set examples are pictures of dogs. So, that is, if 5 out of 100 of these mislabeled dev set examples are dogs. Of a typical set of 100 examples you’re getting wrong, even if you completely solve the dog problem, you only get 5 out of 100 more correct. Or in other words, if only 5% of your errors are due to dog pictures, then the best you could easily hope to do is only 5% improvement, if you spend a lot of time on the dog problem. However, if 50 out of 100 of these mislabeled dev set examples are dogs, then 50% of your errors are due to dog pictures, in this case, it might be worth to spend time on the dog problem. </p>\n<p>One could evaluate multiple ideas in parallel. For example, we have some ideas for the improvement of cat detection. And from the percentage of the total mis-classified images, 54% of them have blurry issue which is at most, it is then advisable to improve performance on blurry images.</p>\n<table>\n<thead>\n<tr>\n<th>Image</th>\n<th>Fix dog</th>\n<th>Fix great cats</th>\n<th>Blurry</th>\n<th>Instagram</th>\n<th>Comments</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>x</td>\n<td></td>\n<td></td>\n<td>x</td>\n<td>pitbull</td>\n</tr>\n<tr>\n<td>2</td>\n<td></td>\n<td></td>\n<td>x</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>3</td>\n<td></td>\n<td></td>\n<td>x</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>4</td>\n<td>…</td>\n<td>…</td>\n<td>…</td>\n<td>…</td>\n<td></td>\n</tr>\n<tr>\n<td>% of total</td>\n<td>5%</td>\n<td><strong>41%</strong></td>\n<td><strong>54%</strong></td>\n<td>15%</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p><strong>Mislabeled exmaples</strong> to refer to if your learning algorithm outputs the wrong value y.\n<strong>Incorrectly labeled examples</strong> to refer to in the dataset you have in the training set or the dev set or the test set, the label for y assigned to this piece of data is incorrect. </p>\n<p>Deep learning algorithms are quite robust to random errors in the training set, but they are less robust to systematic errors. So in general if the incorrectly labeled data are random, then you don’t have to fix those incorrect labels, however, if they are systematic, then you should check and fix them. </p>\n<p>If the incorrect labels are in dev and test set, we can add an extra column called “Incorrectly labeled” to identify them in the error analysis. Apple same process to your dev and test sets to make sure they continue to come from the same distribution. Consider examing examples your algorithm got right as well as ones it got wrong. Notice that, if you go to dev and test data to correct some of the mislabels there, the train and dev/test data may now come from slightly different distribution.</p>\n<h2 id=\"build-your-first-system-quickly-then-iterate\" style=\"position:relative;\"><a href=\"#build-your-first-system-quickly-then-iterate\" aria-label=\"build your first system quickly then iterate permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Build your first system quickly, then iterate</h2>\n<p>In speech recognition example, many issues to tackle: noisy background, accent speech, far from microphone, young children’s speech, stuttering … if you need to build a brand new machine learning application here, you should build your first system quickly and then iterate. First, quickly set dev/test set and its metric; Second, build the intial system quickly; Third, use Bias/Variance analysis and error analysis to prioritize next steps.</p>\n<h2 id=\"biasvariance-on-mismatched-training-and-devtest-sets\" style=\"position:relative;\"><a href=\"#biasvariance-on-mismatched-training-and-devtest-sets\" aria-label=\"biasvariance on mismatched training and devtest sets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Bias/variance on mismatched training and dev/test sets</h2>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Human level error</td>\n<td>3%</td>\n<td></td>\n</tr>\n<tr>\n<td>Training set error</td>\n<td>8%</td>\n<td>avoidable bias</td>\n</tr>\n<tr>\n<td>Training-dev set error</td>\n<td>10%</td>\n<td>variance</td>\n</tr>\n<tr>\n<td>Dev error</td>\n<td>12%</td>\n<td>data mismatch</td>\n</tr>\n<tr>\n<td>Test error</td>\n<td>13%</td>\n<td>degrees of over-fitting</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"transfer-learning\" style=\"position:relative;\"><a href=\"#transfer-learning\" aria-label=\"transfer learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transfer learning</h2>\n<p>Artifical data synthesis, pre-training, fine-tuning, transfer learning. Transfer learning will make sense if you have reasonable large data in your pre-training set, and you have quite small data in your fine-tuning set. In general the transfer learning from Task A to Task B makes sense if: Task A and B have the same input X; You have a lot more data in A than in B; low level features from A could be helpful for learning B.</p>\n<h2 id=\"multi-task-learning\" style=\"position:relative;\"><a href=\"#multi-task-learning\" aria-label=\"multi task learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi-task learning</h2>\n<p>When multi-tasking learning make sense: Training on a set of tasks that could benefit from having shared lower-level features; Amount of data you have for each task is quite similar; can train a big enough neural network to do well on all tasks. Otherwise, train a separate neural network for each task.</p>\n<h2 id=\"end-to-end-deep-learning\" style=\"position:relative;\"><a href=\"#end-to-end-deep-learning\" aria-label=\"end to end deep learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>End-to-end deep learning</h2>\n<p>There have been some data processing systems, or learning systems that require multiple stages of processing. And what end-to-end deep learning does, is that it can take all those multiple stages, and replace it usually with just a single neural network. </p>\n<p>Pros: Let the data speak. Less hand-designing of components needed. Cons: May need large amount data. Exclude potentially useful hand-designed components. Data and hand-design. Hand-design allows humans to inject a lot of knowledge about the problem. Key question: Do you have sufficient data to learn a function of complexity needed to map from X to Y?</p>","fields":{"slug":"/posts/structuring-ML-projects","tagSlugs":["/tag/machine-learning/"]},"frontmatter":{"date":"2021-02-27T22:12","description":"Notes taken from the AI course by Andrew Ng.","tags":["Machine Learning"],"title":"Structuring Machine Learning Projects","socialImage":{"publicURL":"/static/3e20438298edd3a3fa746766d803f379/gutenberg.jpg"}}}},"pageContext":{"slug":"/posts/structuring-ML-projects"}},"staticQueryHashes":["251939775","401334301","825871152"]}