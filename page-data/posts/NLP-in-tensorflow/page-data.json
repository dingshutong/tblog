{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/NLP-in-tensorflow","result":{"data":{"markdownRemark":{"id":"f3b21d88-13ba-5b5d-8c78-397222b068ce","html":"<h2 id=\"sentiment-in-text\" style=\"position:relative;\"><a href=\"#sentiment-in-text\" aria-label=\"sentiment in text permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Sentiment in Text</h2>\n<h3 id=\"word-indexing\" style=\"position:relative;\"><a href=\"#word-indexing\" aria-label=\"word indexing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Word Indexing</h3>\n<p>The encoding for natural language is not based on letters, but based on words. Tensorflow and keras provide APIs to encode words.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> Tokenizer\n\nsentences <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token string\">'i love my dog'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'You, love your cat!'</span>\n<span class=\"token punctuation\">]</span>\n\ntokenizer <span class=\"token operator\">=</span> Tokenizer<span class=\"token punctuation\">(</span>num_words <span class=\"token operator\">=</span> <span class=\"token number\">100</span><span class=\"token punctuation\">)</span>\ntokenizer<span class=\"token punctuation\">.</span>fit_on_texts<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span>\nword_index <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">.</span>word_index\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>word_index<span class=\"token punctuation\">)</span></code></pre></div>\n<p>{‘love’: 1, ‘i’: 2, ‘my’: 3, ‘dog’: 4, ‘you’: 5, ‘your’: 6, ‘cat’: 7}</p>\n<h3 id=\"word-to-sequence\" style=\"position:relative;\"><a href=\"#word-to-sequence\" aria-label=\"word to sequence permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Word to Sequence</h3>\n<p>The tokenizer is able to encode the words and sentences and to build up a dictionary of all the words (word indexing), then it turns your sentences into lists of values based on these tokens.  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">sequences <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">.</span>texts_to_sequences<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>sequences<span class=\"token punctuation\">)</span>\n\nnew_sentences <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token string\">\"I don't know you\"</span><span class=\"token punctuation\">,</span>              \n              <span class=\"token string\">\"I love your cat\"</span>   \n<span class=\"token punctuation\">]</span>\n\nnewSeq <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">.</span>texts_to_sequences<span class=\"token punctuation\">(</span>new_sentences<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>newSeq<span class=\"token punctuation\">)</span></code></pre></div>\n<p>[[2, 1, 3, 4], [5, 1, 6, 7]]</p>\n<p>[[2, 5], [2, 1, 6, 7]]</p>\n<p>Notice that in the new sentences, if any word didn’t get trained and indexed, it will be omitted in the sequence indexing, as in <strong>“I don’t know you”</strong>.</p>\n<h3 id=\"placeholder-for-missing-word\" style=\"position:relative;\"><a href=\"#placeholder-for-missing-word\" aria-label=\"placeholder for missing word permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Placeholder for missing word</h3>\n<p>We can introduce a special token for missing word while specify the training set, for example, a token ”<OOV>” (Out Of Vocabulary) to indicate any missing word in the later prediction.  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> Tokenizer\n\nsentences <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token string\">'i love my dog'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'You, love your cat!'</span>\n<span class=\"token punctuation\">]</span>\n\ntokenizer <span class=\"token operator\">=</span> Tokenizer<span class=\"token punctuation\">(</span>num_words <span class=\"token operator\">=</span> <span class=\"token number\">100</span><span class=\"token punctuation\">,</span> oov_token <span class=\"token operator\">=</span> <span class=\"token string\">\"&lt;OOV>\"</span><span class=\"token punctuation\">)</span>\ntokenizer<span class=\"token punctuation\">.</span>fit_on_texts<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span>\nword_index <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">.</span>word_index\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>word_index<span class=\"token punctuation\">)</span></code></pre></div>\n<p>{’<OOV>’: 1, ‘love’: 2, ‘i’: 3, ‘my’: 4, ‘dog’: 5, ‘you’: 6, ‘your’: 7, ‘cat’: 8}</p>\n<p>The new sequence will be </p>\n<p>[[3, 1, 1, 6], [3, 2, 7, 8]]</p>\n<h3 id=\"padding\" style=\"position:relative;\"><a href=\"#padding\" aria-label=\"padding permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Padding</h3>\n<p>In order to work all the sentences with the uniform size (the same length), we can use <strong>padding</strong> API in keras to pad the sequences into same length.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token decorator annotation punctuation\">@keras_export</span><span class=\"token punctuation\">(</span><span class=\"token string\">'keras.preprocessing.sequence.pad_sequences'</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">pad_sequences</span><span class=\"token punctuation\">(</span>sequences<span class=\"token punctuation\">,</span> maxlen<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span><span class=\"token string\">'int32'</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'pre'</span><span class=\"token punctuation\">,</span> truncating<span class=\"token operator\">=</span><span class=\"token string\">'pre'</span><span class=\"token punctuation\">,</span> value<span class=\"token operator\">=</span><span class=\"token number\">0.0</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>In order to use the padding functions you’ll have to import pad sequences from <strong>tensorflow.keras.preprocessing.sequence</strong>.</p>\n<h2 id=\"an-example-of-text-data-preprocessing\" style=\"position:relative;\"><a href=\"#an-example-of-text-data-preprocessing\" aria-label=\"an example of text data preprocessing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>An example of text data preprocessing</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> json\n\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"/tmp/sarcasm.json\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'r'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    datastore <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">)</span>\n\n\nsentences <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span> \nlabels <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\nurls <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> item <span class=\"token keyword\">in</span> datastore<span class=\"token punctuation\">:</span>\n    sentences<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">[</span><span class=\"token string\">'headline'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    labels<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">[</span><span class=\"token string\">'is_sarcastic'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    urls<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">[</span><span class=\"token string\">'article_link'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n\n\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> Tokenizer\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>sequence <span class=\"token keyword\">import</span> pad_sequences\ntokenizer <span class=\"token operator\">=</span> Tokenizer<span class=\"token punctuation\">(</span>oov_token<span class=\"token operator\">=</span><span class=\"token string\">\"&lt;OOV>\"</span><span class=\"token punctuation\">)</span>\ntokenizer<span class=\"token punctuation\">.</span>fit_on_texts<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span>\n\nword_index <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">.</span>word_index\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>word_index<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>word_index<span class=\"token punctuation\">)</span>\nsequences <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">.</span>texts_to_sequences<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span>\npadded <span class=\"token operator\">=</span> pad_sequences<span class=\"token punctuation\">(</span>sequences<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'post'</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>padded<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>padded<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<p>[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n0     0     0     0     0     0     0     0     0     0     0     0\n0     0     0     0     0     0     0     0     0     0     0     0\n0     0     0     0]</p>\n<p>(26709, 40)</p>","fields":{"slug":"/posts/NLP-in-tensorflow","tagSlugs":["/tag/machine-learning/","/tag/natural-language-processing/","/tag/tensorflow/"]},"frontmatter":{"date":"2021-02-28T22:12","description":"Notes taken from the TensorFlow course by Laurence Moroney.","tags":["Machine Learning","Natural Language Processing","Tensorflow"],"title":"Natural Language Processing in TensorFlow","socialImage":{"publicURL":"/static/3e20438298edd3a3fa746766d803f379/gutenberg.jpg"}}}},"pageContext":{"slug":"/posts/NLP-in-tensorflow"}},"staticQueryHashes":["251939775","401334301","825871152"]}